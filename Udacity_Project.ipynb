{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db53530f",
   "metadata": {},
   "source": [
    "# Developer Salary Prediction\n",
    "\n",
    "This project aims to predict a developer's annual income using the Stack Overflow Developer Survey. The goal is to apply a data science process (CRISP-DM) to\n",
    "build a model that can estimate a respondent’s `ConvertedCompYearly` (annual compensation) based on their survey responses. Accurate salary predictions can help developers benchmark compensation and understand how factors like experience, education, and technology usage influence pay.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fcf360",
   "metadata": {},
   "source": [
    "## 1. Business Understanding\n",
    "\n",
    "The primary objective of this project is to build a regression model that predicts a survey respondent’s annual salary. With insights from the Stack Overflow Developer Survey, we hope to answer questions such as:\n",
    "\n",
    "* **Which factors most strongly influence salary?** For example, years of professional coding experience, education level, and technology stacks may all contribute.\n",
    "* **How large is the salary gap across different developer populations?** Understanding salary distributions helps identify outliers and inequities.\n",
    "* **Can we build a reliable model to estimate salaries for unknown cases?** A well-performing model can be used by developers to benchmark their compensation against peers.\n",
    "\n",
    "We follow the CRISP-DM methodology, which structures a data science project into the following stages: business understanding, data understanding, data preparation, modeling, evaluation, and deployment. This notebook focuses on the first five steps, leaving deployment as a potential extension.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5129b4b",
   "metadata": {},
   "source": [
    "## 2. Data Understanding\n",
    "\n",
    "In this section we explore the Stack Overflow survey data. The raw dataset includes responses from thousands of developers across the world, with hundreds of questions covering demographics, education, technology use, and compensation.\n",
    "\n",
    "We start by loading the survey results and inspecting the structure of the dataset. We also examine missing values and initial statistics to get a sense of the data quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7b15ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv('survey_results_public.csv')\n",
    "    print(f\"Data loaded: {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Survey data file not found. Please ensure 'survey_results_public.csv' is available in this directory.\")\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "if not df.empty:\n",
    "    df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97de2f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    display(df[numeric_cols].describe())\n",
    "\n",
    "    missing_percent = df.isna().mean().sort_values(ascending=False) * 100\n",
    "    missing_df = missing_percent.to_frame(name='MissingPercent')\n",
    "    display(missing_df.head(20))\n",
    "\n",
    "    if 'ConvertedCompYearly' in df.columns:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        sns.boxplot(x=df['ConvertedCompYearly'], ax=axes[0])\n",
    "        axes[0].set_title('Boxplot of Annual Salary (ConvertedCompYearly)')\n",
    "        axes[0].set_xlabel('Salary (USD)')\n",
    "\n",
    "        sns.histplot(df['ConvertedCompYearly'].dropna(), kde=True, ax=axes[1])\n",
    "        axes[1].set_title('Histogram of Annual Salary (ConvertedCompYearly)')\n",
    "        axes[1].set_xlabel('Salary (USD)')\n",
    "\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46b0410",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "The raw data contains many columns that are either irrelevant to salary prediction or have a very high percentage of missing values. In this step we clean the data by:\n",
    "\n",
    "* **Removing rows without salary information.** Our target variable is `ConvertedCompYearly`, so rows without this value cannot be used for modeling.\n",
    "* **Dropping columns with too many missing values.** Columns with more than 30% missing data are discarded to simplify the analysis.\n",
    "* **Converting ordinal categorical variables to numeric values.** For example, we convert `YearsCode` and `YearsCodePro` into numeric years of experience.\n",
    "* **Handling multi-select columns.** Some survey questions allow respondents to select multiple values separated by semicolons. We can expand these into binary indicator columns. For simplicity, this notebook demonstrates the function but does not apply it to all multi-valued columns.\n",
    "* **Removing extreme outliers.** We use the interquartile range (IQR) method to drop unreasonable salary values that could skew the model.\n",
    "\n",
    "These steps produce a clean, numeric feature set suitable for machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4186d6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original DataFrame to avoid modifying the raw data\n",
    "clean_df = df.copy()\n",
    "\n",
    "if not clean_df.empty and 'ConvertedCompYearly' in clean_df.columns:\n",
    "# Remove entries where salary (ConvertedCompYearly) is missing\n",
    "    clean_df = clean_df[clean_df['ConvertedCompYearly'].notna()].reset_index(drop=True)\n",
    "    print(f\"Rows with salary: {len(clean_df)} (removed {len(df) - len(clean_df)} rows without salary)\")\n",
    "\n",
    "if not clean_df.empty:\n",
    "# Define a threshold (30%) for dropping columns with too many missing values\n",
    "    missing_threshold = 0.3\n",
    "# Define a threshold (30%) for dropping columns with too many missing values\n",
    "    cols_to_drop = [col for col in clean_df.columns if clean_df[col].isna().mean() > missing_threshold]\n",
    "# Identify columns exceeding the missing value threshold\n",
    "    print(f\"Dropping {len(cols_to_drop)} columns with >30% missing values\")\n",
    "# Identify columns exceeding the missing value threshold\n",
    "    clean_df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "if not clean_df.empty:\n",
    "    import numpy as np\n",
    "# Define helper function to convert years of experience strings to integers\n",
    "    def years_to_int(x):\n",
    "        x = str(x).strip()\n",
    "        if x == 'More than 50 years':\n",
    "            return 60\n",
    "        if x == 'Less than 1 year':\n",
    "            return 0\n",
    "        try:\n",
    "            return int(float(x))\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "# Apply the conversion function to YearsCode and YearsCodePro columns\n",
    "    for col in ['YearsCode', 'YearsCodePro']:\n",
    "        if col in clean_df.columns:\n",
    "            clean_df[col] = clean_df[col].apply(years_to_int)\n",
    "    if 'YearsCode' in clean_df.columns and 'YearsCodePro' in clean_df.columns:\n",
    "# Use YearsCodePro to fill missing YearsCode and vice versa\n",
    "        clean_df['YearsCode'].fillna(clean_df['YearsCodePro'], inplace=True)\n",
    "# Use YearsCodePro to fill missing YearsCode and vice versa\n",
    "        clean_df['YearsCodePro'].fillna(clean_df['YearsCode'], inplace=True)\n",
    "\n",
    "if not clean_df.empty:\n",
    "    before_rows = len(clean_df)\n",
    "# Drop rows with missing experience after conversion\n",
    "    clean_df = clean_df.dropna(subset=['YearsCode', 'YearsCodePro'])\n",
    "    print(f\"Dropped {before_rows - len(clean_df)} rows due to missing experience\")\n",
    "# Convert years columns to integer type after filling missing values\n",
    "    clean_df['YearsCode'] = clean_df['YearsCode'].astype(int)\n",
    "# Convert years columns to integer type after filling missing values\n",
    "    clean_df['YearsCodePro'] = clean_df['YearsCodePro'].astype(int)\n",
    "\n",
    "if not clean_df.empty:\n",
    "# Calculate the IQR bounds to identify outliers in salary\n",
    "    Q1 = clean_df['ConvertedCompYearly'].quantile(0.25)\n",
    "# Calculate the IQR bounds to identify outliers in salary\n",
    "    Q3 = clean_df['ConvertedCompYearly'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "# Keep only rows within the calculated salary bounds to remove outliers\n",
    "    clean_df = clean_df[(clean_df['ConvertedCompYearly'] >= lower_bound) & (clean_df['ConvertedCompYearly'] <= upper_bound)]\n",
    "    print(f\"Data after outlier removal: {clean_df.shape[0]} rows remaining\")\n",
    "\n",
    "clean_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb2a7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_multivalue_columns(df, columns):\n",
    "    '''\n",
    "    Expand multi-select columns (semicolon-separated) into binary indicator columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The original dataframe.\n",
    "    columns : list of str\n",
    "        Column names to expand.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The dataframe with expanded columns appended.\n",
    "    '''\n",
    "    df_expanded = df.copy()\n",
    "    for col in columns:\n",
    "        if col not in df_expanded.columns:\n",
    "            continue\n",
    "        if not df_expanded[col].dropna().astype(str).str.contains(';').any():\n",
    "            continue\n",
    "        split_vals = df_expanded[col].fillna('').astype(str).str.split(';')\n",
    "        unique_vals = set([item.strip() for sublist in split_vals for item in sublist if item])\n",
    "        for val in unique_vals:\n",
    "            new_col = f\"{col}__{val}\"\n",
    "            df_expanded[new_col] = split_vals.apply(lambda x: int(val in [item.strip() for item in x]))\n",
    "        df_expanded.drop(columns=[col], inplace=True)\n",
    "    return df_expanded\n",
    "\n",
    "# Example usage (not executed for all columns to avoid blow-up)\n",
    "# multi_cols = ['LanguageHaveWorkedWith', 'DatabaseHaveWorkedWith']\n",
    "# clean_df = expand_multivalue_columns(clean_df, multi_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc52484a",
   "metadata": {},
   "source": [
    "## 4. Exploratory Questions and Answers\n",
    "\n",
    "Before building predictive models, it's important to explore the cleaned data and answer business-oriented questions that provide context. Here we pose several questions and answer them using visualizations and summary statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a07f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1: How does the number of years a respondent has been coding relate to their salary?\n",
    "if not clean_df.empty:\n",
    "    # Scatter plot of years of experience vs salary\n",
    "    sns.scatterplot(x=clean_df['YearsCode'], y=clean_df['ConvertedCompYearly'])\n",
    "    plt.title('Years of Coding Experience vs Annual Salary')\n",
    "    plt.xlabel('Years of Coding Experience')\n",
    "    plt.ylabel('Annual Salary (USD)')\n",
    "    plt.show()\n",
    "    \n",
    "    # Compute and display correlation coefficient\n",
    "    corr = clean_df[['YearsCode', 'ConvertedCompYearly']].corr().iloc[0, 1]\n",
    "    print(f'Correlation between coding experience and salary: {corr:.2f}')\n",
    "else:\n",
    "    print('Clean dataset is empty; cannot compute this analysis.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d005b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2: What is the distribution of salaries by respondents' education level?\n",
    "if not clean_df.empty and 'EdLevel' in clean_df.columns:\n",
    "    # Compute median salary per education level\n",
    "    edu_salary = clean_df.groupby('EdLevel')['ConvertedCompYearly'].median().sort_values(ascending=False)\n",
    "    display(edu_salary.head())\n",
    "    # Bar plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=edu_salary.index, y=edu_salary.values)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title('Median Salary by Education Level')\n",
    "    plt.ylabel('Median Salary (USD)')\n",
    "    plt.xlabel('Education Level')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Education level column not found in the dataset.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61bdb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3: Does company size impact salary?\n",
    "if not clean_df.empty and 'OrgSize' in clean_df.columns:\n",
    "    # Compute median salary per organization size category\n",
    "    size_salary = clean_df.groupby('OrgSize')['ConvertedCompYearly'].median().sort_values(ascending=False)\n",
    "    display(size_salary)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(x=size_salary.index, y=size_salary.values)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title('Median Salary by Company Size')\n",
    "    plt.ylabel('Median Salary (USD)')\n",
    "    plt.xlabel('Company Size')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Company size column not found in the dataset.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed2e35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4: Which countries have the highest average salary? (Top 10)\n",
    "if not clean_df.empty and 'Country' in clean_df.columns:\n",
    "    # Compute average salary per country and select top 10\n",
    "    country_salary = clean_df.groupby('Country')['ConvertedCompYearly'].mean().sort_values(ascending=False).head(10)\n",
    "    display(country_salary)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=country_salary.values, y=country_salary.index)\n",
    "    plt.title('Top 10 Countries by Average Salary')\n",
    "    plt.xlabel('Average Salary (USD)')\n",
    "    plt.ylabel('Country')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Country column not found in the dataset.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c336dc5",
   "metadata": {},
   "source": [
    "## 5. Modeling\n",
    "\n",
    "With a cleaned dataset, we can now build machine learning models to predict annual salary. We start by splitting the data into training and testing sets. Then we apply one-hot encoding to categorical variables and scaling to numeric variables using a `ColumnTransformer`. Finally, we train two models:\n",
    "\n",
    "1. **Linear Regression** - a simple baseline model.\n",
    "2. **Random Forest Regressor** - an ensemble tree-based model that can capture nonlinear relationships and interactions.\n",
    "\n",
    "We evaluate model performance using the mean squared error (MSE), mean absolute error (MAE), and the coefficient of determination (R2). The model with the best metrics will be selected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a5270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "if not clean_df.empty:\n",
    "# Separate the target variable (salary) from features\n",
    "    y = clean_df['ConvertedCompYearly']\n",
    "    X = clean_df.drop(columns=['ConvertedCompYearly'])\n",
    "# Identify categorical columns for one-hot encoding\n",
    "    categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "# Identify numeric columns for scaling\n",
    "    numeric_cols = X.select_dtypes(exclude=['object']).columns.tolist()\n",
    "# Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Define preprocessing: scale numeric features and one-hot encode categorical features\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "# Identify numeric columns for scaling\n",
    "            ('num', StandardScaler(), numeric_cols),\n",
    "# Identify categorical columns for one-hot encoding\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "        ])\n",
    "# Define the machine learning models to compare\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "    }\n",
    "    results = {}\n",
    "# Train and evaluate each model\n",
    "    for name, model in models.items():\n",
    "# Create a pipeline that applies preprocessing then fits the model\n",
    "        clf = Pipeline(steps=[\n",
    "# Define preprocessing: scale numeric features and one-hot encode categorical features\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', model)\n",
    "        ])\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "# Compute mean squared error\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "# Compute mean absolute error\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "# Compute coefficient of determination (R2)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        results[name] = {\n",
    "            'MSE': mse,\n",
    "            'MAE': mae,\n",
    "            'R2': r2\n",
    "        }\n",
    "    results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60f2de2",
   "metadata": {},
   "source": [
    "## 6. Feature Importance via SHAP (CatBoost)\n",
    "\n",
    "Before selecting our final model, we examine feature importance using SHAP values derived from a CatBoost regressor. SHAP values quantify the contribution of each feature to the model’s predictions. Here we train a CatBoost model using the cleaned data and compute the mean absolute SHAP value for each feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9adab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "if not clean_df.empty:\n",
    "    # Separate target and features\n",
    "    y_shap = clean_df['ConvertedCompYearly']\n",
    "    X_shap = clean_df.drop(columns=['ConvertedCompYearly'])\n",
    "    \n",
    "    # Identify indices of categorical features for CatBoost\n",
    "    categorical_features = [i for i, col in enumerate(X_shap.columns) if X_shap[col].dtype == 'object']\n",
    "    \n",
    "    # Initialize CatBoostRegressor with modest hyperparameters for demonstration\n",
    "    cat_model = CatBoostRegressor(\n",
    "        iterations=200,\n",
    "        learning_rate=0.1,\n",
    "        depth=8,\n",
    "        loss_function='RMSE',\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Fit the model (CatBoost automatically handles categorical features)\n",
    "    cat_model.fit(X_shap, y_shap, cat_features=categorical_features)\n",
    "    \n",
    "    # Compute SHAP values; the last column of the array is the expected value so exclude it\n",
    "    shap_values = cat_model.get_feature_importance(Pool(X_shap, y_shap, cat_features=categorical_features), type='ShapValues')\n",
    "    shap_values = shap_values[:, :-1]\n",
    "    \n",
    "    # Calculate mean absolute SHAP value for each feature\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    feature_importance = np.abs(shap_values).mean(axis=0)\n",
    "    shap_importance = pd.Series(feature_importance, index=X_shap.columns).sort_values(ascending=False)\n",
    "    \n",
    "    # Display top 20 features\n",
    "    top_features = shap_importance.head(20)\n",
    "    display(top_features)\n",
    "    \n",
    "    # Plot top features\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=top_features.values, y=top_features.index)\n",
    "    plt.title('Top 20 Features by Mean Absolute SHAP Value')\n",
    "    plt.xlabel('Mean |SHAP value|')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Clean dataset is empty. Cannot compute SHAP values.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93a5c14",
   "metadata": {},
   "source": [
    "## 7. Evaluation\n",
    "\n",
    "Once the models are trained, we compare their performance on the test set. A lower mean squared error and mean absolute error indicate better predictive accuracy, while a higher R2 score means the model explains more variance in the target variable.\n",
    "\n",
    "Use the results dictionary from the previous step to examine which model performs best. It is common for tree-based ensemble models like random forests to outperform simple linear regression when relationships between features and salary are nonlinear.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420bed4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'results' in globals():\n",
    "    import pandas as pd\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0e160a",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "In this project we applied the CRISP-DM methodology to build a salary prediction model using Stack Overflow survey data. We began with business understanding to clarify the objectives and identify potential variables influencing compensation. During data understanding and preparation we investigated the structure of the dataset, cleaned missing values, transformed experience into numeric formats, removed outliers, and prepared features for modeling.\n",
    "\n",
    "Our exploratory data analysis revealed skewed salary distributions and highlighted the importance of cleaning extreme values. The modeling step compared a baseline linear regression model with a random forest regressor. Evaluation metrics (MSE, MAE and R2) indicated which model offered better predictive performance. While further improvements are possible (e.g., hyperparameter tuning, incorporating more features or advanced models like XGBoost or CatBoost), the random forest provided a solid starting point.\n",
    "\n",
    "Future work might involve:\n",
    "\n",
    "* Tuning the hyperparameters of tree-based models to improve accuracy.\n",
    "* Exploring additional features such as education level, geographic location, and technology stacks.\n",
    "* Deploying the best model as a web service or interactive tool for developers to estimate their salary.\n",
    "\n",
    "By following a structured process and incorporating robust EDA and modeling techniques, we can derive meaningful insights and build predictive tools that help developers understand compensation trends.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
